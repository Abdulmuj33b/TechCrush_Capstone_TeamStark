{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdulmuj33b/TechCrush_Capstone_TeamStark/blob/main/Heart_Disease.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8ef6c68",
      "metadata": {
        "id": "b8ef6c68"
      },
      "source": [
        "# 01 - Exploratory Data Analysis (EDA)\n",
        "\n",
        "- Loads `heart.xls` from `/mnt/data`\n",
        "- Cleans and summarizes the dataset\n",
        "- Produces visualizations (displayed inline) and saves them to `images/`\n",
        "- Performs simple statistical tests (t-test for numeric vs target, chi-square for categorical vs target)\n",
        "- Prints short, insights after each analysis step\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1LL9wp_7YK9",
        "outputId": "a88278fa-0c6f-47ea-87fe-44c68dbb5f0c"
      },
      "id": "S1LL9wp_7YK9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a195293b",
        "outputId": "47404656-e0f2-48f0-d714-1024cf758155"
      },
      "source": [
        "!pip install optuna"
      ],
      "id": "a195293b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.0)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0edd48b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0edd48b",
        "outputId": "4d95fb28-aa8f-4f09-a345-af333e49a10d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images will be saved to: /content/MyDrive/heart-disease-project/images\n"
          ]
        }
      ],
      "source": [
        "# Standard imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import statsmodels.api as sm\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, classification_report, RocCurveDisplay, PrecisionRecallDisplay\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib\n",
        "from IPython.display import display\n",
        "\n",
        "# Make plots look nicer\n",
        "%matplotlib inline\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "# Paths - consolidated\n",
        "DATA_PATH = '/content/MyDrive/heart-disease-project/data/heart.xls'\n",
        "CLEAN_PATH = '/content/MyDrive/heart-disease-project/data/cleaned.heart.csv'\n",
        "IMAGES_DIR = os.path.join('/content/MyDrive/heart-disease-project/images')\n",
        "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
        "print('Images will be saved to:', IMAGES_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86b2bd02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "86b2bd02",
        "outputId": "00ea4a79-b5fe-4dd1-8660-0c83f82faec7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/MyDrive/heart-disease-project/data/heart.xls'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3336239113.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1. Load the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# It appears the file is actually a CSV despite the .xls extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Initial shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/MyDrive/heart-disease-project/data/heart.xls'"
          ]
        }
      ],
      "source": [
        "# 1. Load the data\n",
        "# It appears the file is actually a CSV despite the .xls extension\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print('Initial shape:', df.shape)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Removed unmount as it's not available in this version\n",
        "# try:\n",
        "#   drive.unmount('/content/drive')\n",
        "# except ValueError:\n",
        "#   pass # Already unmounted or not mounted\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "an1xw-SEedMw",
        "outputId": "9040444d-a4cb-41ef-b90c-f2d890615576"
      },
      "id": "an1xw-SEedMw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Mountpoint must not already contain files",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3414620502.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# except ValueError:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#   pass # Already unmounted or not mounted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a328cc50"
      },
      "source": [
        "!pip install xlrd"
      ],
      "id": "a328cc50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c972f6c",
      "metadata": {
        "id": "7c972f6c"
      },
      "outputs": [],
      "source": [
        "# 2. Basic cleaning function (lowercase cols, strip whitespace, replace empty strings)\n",
        "def basic_clean(df):\n",
        "    df = df.copy()\n",
        "    df.columns = [str(c).strip().lower().replace(' ', '_') for c in df.columns]\n",
        "    df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "    return df\n",
        "\n",
        "df = basic_clean(df)\n",
        "print('After basic cleaning shape:', df.shape)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14f1276",
      "metadata": {
        "id": "b14f1276"
      },
      "outputs": [],
      "source": [
        "# Identify duplicate rows\n",
        "duplicate_rows = df[df.duplicated()]\n",
        "\n",
        "# Display the head of the duplicate rows\n",
        "print(\"Head of duplicate rows:\")\n",
        "display(duplicate_rows.head())\n",
        "\n",
        "# Print the total number of duplicate rows\n",
        "print(\"\\nTotal number of duplicate rows:\", duplicate_rows.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_RtyN5v7mUor",
      "metadata": {
        "id": "_RtyN5v7mUor"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0fc06cd2",
      "metadata": {
        "id": "0fc06cd2"
      },
      "source": [
        "## Why We Kept Duplicates in This Health Dataset\n",
        "\n",
        "We did not remove duplicate rows in this health dataset because:\n",
        "\n",
        "*   **Multiple visits/events:** Patients can have multiple records representing different visits or medical events.\n",
        "*   **Different observations:** Multiple valid observations might exist for the same patient at a given time.\n",
        "*   **Data nuances:** The data collection process can result in seemingly duplicate, but valid, entries.\n",
        "\n",
        "Keeping these rows preserves the full patient history and avoids losing important information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f577417",
      "metadata": {
        "id": "6f577417"
      },
      "outputs": [],
      "source": [
        "# Ensure target exists and is binary\n",
        "if 'target' not in df.columns:\n",
        "    raise KeyError(\"The dataset must contain a 'target' column with 0/1 values indicating heart disease.\")\n",
        "print('Target unique values:', df['target'].unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9d0c22f",
      "metadata": {
        "id": "a9d0c22f"
      },
      "source": [
        "## 3. Quick summary & missingness\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46ef6d1f",
      "metadata": {
        "id": "46ef6d1f"
      },
      "outputs": [],
      "source": [
        "display(df.info())\n",
        "display(df.describe(include='all').T)\n",
        "\n",
        "# Missing values summary\n",
        "missing = df.isnull().sum().sort_values(ascending=False)\n",
        "missing = missing[missing>0]\n",
        "print('\\nColumns with missing values:\\n')\n",
        "print(missing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b54c3bc2",
      "metadata": {
        "id": "b54c3bc2"
      },
      "source": [
        "## 4. Class distribution (target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7883b4",
      "metadata": {
        "id": "4e7883b4"
      },
      "outputs": [],
      "source": [
        "ax = df['target'].value_counts().sort_index().plot(kind='bar', color=['blue', 'red'])\n",
        "ax.set_xticklabels(['No Disease (0)','Disease (1)'], rotation=0)\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Target class distribution')\n",
        "plt.tight_layout()\n",
        "fn = os.path.join(IMAGES_DIR, 'target_distribution.png')\n",
        "plt.savefig(fn, dpi=150)\n",
        "plt.show()\n",
        "print('\\nQuick-insight:')\n",
        "counts = df['target'].value_counts().sort_index()\n",
        "if counts.shape[0]==2:\n",
        "    pct_disease = counts.loc[1] / counts.sum()\n",
        "    print(f\"Proportion with heart disease: {pct_disease:.2%} (class=1).\")\n",
        "    pct_no_disease = counts.loc[0] / counts.sum()\n",
        "    print(f\"Proportion without heart disease: {pct_no_disease:.2%} (class=0).\")\n",
        "    print(\"This indicates whether class imbalance handling may be needed.\")\n",
        "else:\n",
        "    print('Target is not binary or unexpected unique values.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "654af08b",
      "metadata": {
        "id": "654af08b"
      },
      "source": [
        "## 5. Identifying numeric and categorical columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1zmupsAowrTW",
      "metadata": {
        "id": "1zmupsAowrTW"
      },
      "source": [
        "This is crucial because it determines how we process, visualize, and prepare data for modeling. Different data types require different methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e12b4b1f",
      "metadata": {
        "id": "e12b4b1f"
      },
      "outputs": [],
      "source": [
        "num_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n",
        "print('Numeric columns:', num_cols)\n",
        "print('Categorical columns:', cat_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2c483eb",
      "metadata": {
        "id": "f2c483eb"
      },
      "source": [
        "## 6. Univariate analysis — numeric features\n",
        "\n",
        "it helps us to understand:\n",
        "\n",
        "    Distribution patterns of individual variables (normal, skewed, uniform)\n",
        "\n",
        "    Central tendency through mean, median, and mode\n",
        "\n",
        "    Data spread via range, variance, and standard deviation\n",
        "\n",
        "    Presence of outliers that may need special treatment\n",
        "\n",
        "    Overall characteristics of each feature in isolation\n",
        "\n",
        "This foundational understanding informs subsequent data preprocessing and guides more complex multivariate analysis.\n",
        "\n",
        "For each numeric feature we will plot a histogram and boxplot, save them, and compute skewness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "827fd193",
      "metadata": {
        "id": "827fd193"
      },
      "outputs": [],
      "source": [
        "for col in num_cols:\n",
        "    fig, axes = plt.subplots(1,2, figsize=(12,4))\n",
        "    sns.histplot(df[col].dropna(), kde=True, ax=axes[0])\n",
        "    axes[0].set_title(f'Histogram of {col}')\n",
        "    sns.boxplot(x=df[col], ax=axes[1])\n",
        "    axes[1].set_title(f'Boxplot of {col}')\n",
        "    plt.tight_layout()\n",
        "    fn = os.path.join(IMAGES_DIR, f'{col}_hist_box.png')\n",
        "    plt.savefig(fn, dpi=150)\n",
        "    plt.show()\n",
        "    skew = df[col].skew()\n",
        "    print(f\"{col} — skewness: {skew:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f37084",
      "metadata": {
        "id": "24f37084"
      },
      "source": [
        "## 7. Bivariate analysis  (Numeric vs target)\n",
        "We now compare distributions of numeric features grouped by `target` and run t-tests (or Mann-Whitney if non-normal).\n",
        "\n",
        "Essentially, we are checking if the distribution or values of a numeric feature are significantly different between the two target classes (heart disease or no heart disease). This gives us insights into which numeric features might be important predictors of the target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32695df4",
      "metadata": {
        "id": "32695df4"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_ind, mannwhitneyu\n",
        "\n",
        "for col in num_cols:\n",
        "    data0 = df[df['target']==0][col].dropna()\n",
        "    data1 = df[df['target']==1][col].dropna()\n",
        "    # plot\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.boxplot(x='target', y=col, data=df)\n",
        "    plt.title(f'{col} by target')\n",
        "    fn = os.path.join(IMAGES_DIR, f'{col}_by_target_box.png')\n",
        "    plt.savefig(fn, dpi=150)\n",
        "    plt.show()\n",
        "    # choose test based on normality (Shapiro) and sample size\n",
        "    use_mw = False\n",
        "    try:\n",
        "        if len(data0) >= 3 and len(data1) >= 3:\n",
        "            p0 = stats.shapiro(data0.sample(min(5000, len(data0))))[1] if len(data0) <= 5000 else 1.0\n",
        "            p1 = stats.shapiro(data1.sample(min(5000, len(data1))))[1] if len(data1) <= 5000 else 1.0\n",
        "            if p0 < 0.05 or p1 < 0.05:\n",
        "                use_mw = True\n",
        "    except Exception:\n",
        "        use_mw = True\n",
        "    if use_mw:\n",
        "        stat, p = mannwhitneyu(data0, data1, alternative='two-sided')\n",
        "        test_name = 'Mann-Whitney U'\n",
        "    else:\n",
        "        stat, p = ttest_ind(data0, data1, nan_policy='omit')\n",
        "        test_name = 'T-test'\n",
        "    print(f\"{col}: {test_name} p-value = {p:.4f}\")\n",
        "    if p < 0.05:\n",
        "        print(f\"  -> Quick insight: Significant difference in {col} between classes (p<{0.05}).\")\n",
        "    else:\n",
        "        print(f\"  -> Quick insight: No significant difference detected for {col} (p={p:.3f}).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97d9ea2d",
      "metadata": {
        "id": "97d9ea2d"
      },
      "source": [
        "## 8. Correlation matrix (numeric features)\n",
        "\n",
        "This helps us see which features are buddies or rivals, and if any are strongly linked to our goal (target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ac42c23",
      "metadata": {
        "id": "9ac42c23"
      },
      "outputs": [],
      "source": [
        "corr = df[num_cols].corr()\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
        "plt.title('Correlation matrix (numeric features)')\n",
        "fn = os.path.join(IMAGES_DIR, 'correlation_matrix.png')\n",
        "plt.savefig(fn, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28616d08",
      "metadata": {
        "id": "28616d08"
      },
      "source": [
        "## 9. Correlation of features with target (point-biserial for numeric)\n",
        "\n",
        "Here is our way to measure the relationship between the continuous (numeric) variables and the binary (two-category) variable, like our target (disease or no disease).\n",
        "A higher absolute value means the numeric feature is a stronger indicator of whether someone has the disease or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8efe1c84",
      "metadata": {
        "id": "8efe1c84"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pointbiserialr\n",
        "corrs = []\n",
        "for col in num_cols:\n",
        "    try:\n",
        "        r, p = pointbiserialr(df['target'].fillna(0), df[col].fillna(df[col].median()))\n",
        "        corrs.append({'feature': col, 'r': r, 'p': p})\n",
        "    except Exception as e:\n",
        "        pass\n",
        "corr_df = pd.DataFrame(corrs).sort_values('r', key=abs, ascending=False)\n",
        "display(corr_df)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(x='r', y='feature', data=corr_df)\n",
        "plt.title('Point-biserial correlation with target')\n",
        "fn = os.path.join(IMAGES_DIR, 'feature_target_correlation.png')\n",
        "plt.savefig(fn, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print('\\nQuick insight: Features with largest absolute correlation (top 5):')\n",
        "print(corr_df.head(5).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11ccc8cd",
      "metadata": {
        "id": "11ccc8cd"
      },
      "source": [
        "## 10. Simple pairwise plots for top features\n",
        "\n",
        "This will be our visual way to see potential interactions and separation power of the top predictors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6077fdc9",
      "metadata": {
        "id": "6077fdc9"
      },
      "outputs": [],
      "source": [
        "top_feats = corr_df['feature'].head(4).tolist()\n",
        "# Remove 'target' from top_feats if it's there, as it will be used for hue\n",
        "if 'target' in top_feats:\n",
        "    top_feats.remove('target')\n",
        "\n",
        "if len(top_feats) >= 2:\n",
        "    # Pass the features to plot on axes and the target for hue\n",
        "    g = sns.pairplot(df[top_feats + ['target']].dropna(), vars=top_feats, hue='target', corner=True)\n",
        "    g.fig.suptitle('Pairwise Plots of Top Features by Target', y=1.02) # Add a main title\n",
        "    fn = os.path.join(IMAGES_DIR, 'pairplot_top_features.png')\n",
        "    plt.savefig(fn, dpi=300, bbox_inches='tight') # Increase DPI for potentially clearer saved image\n",
        "    plt.show()\n",
        "else:\n",
        "    print('Not enough top numeric features for pairplot.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x4hOkOd2DSnD",
      "metadata": {
        "id": "x4hOkOd2DSnD"
      },
      "source": [
        "Based on the above scatter plots of top key features (oldpeak, exang, cp, thalach):\n",
        "\n",
        "    Clear visual separation exists between heart disease cases (red) and healthy individuals (blue)\n",
        "\n",
        "    Distinct clustering patterns show these features effectively differentiate between disease states\n",
        "\n",
        "    The observed separation validates earlier correlation findings\n",
        "\n",
        "    These visual patterns confirm these variables' importance for predicting heart disease\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449000de",
      "metadata": {
        "id": "449000de"
      },
      "source": [
        "## 11. Outlier detection using IQR (InterQuartile Range) method and counts\n",
        "\n",
        "Identifying outliers allows will us to understand their nature and decide on appropriate strategies, such as investigating their source, removing it, transforming the data, or using models that are more robust to outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9b37e47",
      "metadata": {
        "id": "e9b37e47"
      },
      "outputs": [],
      "source": [
        "outlier_counts = {}\n",
        "for col in num_cols:\n",
        "    q1 = df[col].quantile(0.25)\n",
        "    q3 = df[col].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5 * iqr\n",
        "    upper = q3 + 1.5 * iqr\n",
        "    oc = df[(df[col] < lower) | (df[col] > upper)].shape[0]\n",
        "    outlier_counts[col] = oc\n",
        "outlier_df = pd.DataFrame.from_dict(outlier_counts, orient='index', columns=['outlier_count']).sort_values('outlier_count', ascending=False)\n",
        "display(outlier_df)\n",
        "print('\\nQuick insight: Columns with many outliers may need robust scaling or capping before we start modeling.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93b66d5c",
      "metadata": {
        "id": "93b66d5c"
      },
      "source": [
        "## 12. Missingness visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944be78f",
      "metadata": {
        "id": "944be78f"
      },
      "outputs": [],
      "source": [
        "ms = df.isnull().sum()\n",
        "if ms.sum() == 0:\n",
        "    print('No missing values detected in the dataset.')\n",
        "else:\n",
        "    ms = ms[ms>0].sort_values(ascending=False)\n",
        "    ms.plot.barh(figsize=(6, max(3, len(ms)*0.4)))\n",
        "    plt.title('Missing values per column')\n",
        "    fn = os.path.join(IMAGES_DIR, 'missing_values.png')\n",
        "    plt.savefig(fn, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "943f1f33",
      "metadata": {
        "id": "943f1f33"
      },
      "source": [
        "## 13. Save a cleaned version of the dataset for modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d0fc98",
      "metadata": {
        "id": "f0d0fc98"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Save the cleaned dataset directly in the current working directory\n",
        "\n",
        "# Assuming your cleaned DataFrame is named df\n",
        "df.to_csv(CLEAN_PATH, index=False)\n",
        "\n",
        "print('✅ Saved cleaned data to:', CLEAN_PATH)\n",
        "\n",
        "# Trigger file download to your local machine\n",
        "# Removed files.download(CLEAN_PATH) as the file is saved directly to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc1fb211",
      "metadata": {
        "id": "dc1fb211"
      },
      "source": [
        "## 14. Final Summary of EDA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37834987",
      "metadata": {
        "id": "37834987"
      },
      "outputs": [],
      "source": [
        "print('--- EDA SUMMARY ---')\n",
        "print(f'Total rows: {df.shape[0]}, Total columns: {df.shape[1]}')\n",
        "print('\\nTop numeric features correlated with target:')\n",
        "display(corr_df.head(10))\n",
        "print('\\nColumns with missing values:')\n",
        "display(df.isnull().sum()[df.isnull().sum()>0])\n",
        "print('\\nColumns with most outliers:')\n",
        "display(outlier_df.head(10))\n",
        "print('\\nRecommendations:')\n",
        "print(' - Handle class imbalance if disease prevalence is low (resampling or class weights).')\n",
        "print(' - Consider log-transform or robust scaling for skewed features.')\n",
        "print(' - Impute missing values (median for numeric, mode for categorical) or use model-based imputers.')\n",
        "print(' - Use SHAP/feature importance after modeling to get clinical explanations.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6abfd445",
      "metadata": {
        "id": "6abfd445"
      },
      "source": [
        "## 15. Data Validation using pandera schema\n",
        "\n",
        "    Ensures data quality and catches issues early\n",
        "\n",
        "    Defines data contracts for structure, types, and value ranges\n",
        "\n",
        "    Prevents analysis errors from invalid/missing data\n",
        "\n",
        "    Validates business rules (e.g., realistic medical ranges)\n",
        "\n",
        "    Supports reliable analytics and model building\n",
        "\n",
        "\n",
        "\n",
        "We are maintaining trustworthy analysis results and production-ready data workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aK8-BM9ZgIfz",
      "metadata": {
        "id": "aK8-BM9ZgIfz"
      },
      "source": [
        "Pandera Validation Turns Our Data Quality from \"hope\" into guarantee"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bfc7c57",
      "metadata": {
        "id": "0bfc7c57"
      },
      "outputs": [],
      "source": [
        "!pip install pandera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09299dc8",
      "metadata": {
        "id": "09299dc8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Data validation with pandera schema\n",
        "try:\n",
        "    import pandera.pandas as pa\n",
        "    from pandera import Column, DataFrameSchema\n",
        "except Exception as e:\n",
        "    raise ImportError('pandera not installed. Please install with `pip install pandera`') from e\n",
        "\n",
        "# Generate schema from df dtypes\n",
        "if 'df' not in globals():\n",
        "    print('df not found. Please ensure df is loaded before running this cell.')\n",
        "else:\n",
        "    col_map = {}\n",
        "    for col, dtype in df.dtypes.items():\n",
        "        if pd.api.types.is_integer_dtype(dtype):\n",
        "            col_map[col] = pa.Column(pa.Int, nullable=df[col].isnull().any())\n",
        "        elif pd.api.types.is_float_dtype(dtype):\n",
        "            col_map[col] = pa.Column(pa.Float, nullable=df[col].isnull().any())\n",
        "        elif pd.api.types.is_bool_dtype(dtype):\n",
        "            col_map[col] = pa.Column(pa.Bool, nullable=df[col].isnull().any())\n",
        "        else:\n",
        "            col_map[col] = pa.Column(pa.String, nullable=df[col].isnull().any())\n",
        "\n",
        "    schema = DataFrameSchema(col_map)\n",
        "    print('Inferred pandera schema:')\n",
        "    print(schema)\n",
        "    # run validation (this will raise if invalid)\n",
        "    validated_df = schema.validate(df, lazy=True)\n",
        "    print('Validation passed. Number of rows:', len(validated_df))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yAhQiBOsKmGM",
      "metadata": {
        "id": "yAhQiBOsKmGM"
      },
      "source": [
        "# MODELLING"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_f22D3UxM8ib",
      "metadata": {
        "id": "_f22D3UxM8ib"
      },
      "source": [
        "## 16. Load cleaned data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "usXEZlwqK7vE",
      "metadata": {
        "id": "usXEZlwqK7vE"
      },
      "outputs": [],
      "source": [
        "# 1. Load cleaned data\n",
        "df = pd.read_csv(CLEAN_PATH)\n",
        "print('Loaded shape:', df.shape)\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. Prepare features and target"
      ],
      "metadata": {
        "id": "au5eJh-hVDCB"
      },
      "id": "au5eJh-hVDCB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMJZAX9xTQyZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "TARGET = 'target'\n",
        "if TARGET not in df.columns:\n",
        "    raise KeyError('Target column not found in cleaned data.')\n",
        "X = df.drop(columns=[TARGET])\n",
        "y = df[TARGET]\n",
        "\n",
        "# Identify numeric and categorical columns simply\n",
        "num_cols = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
        "print('Numeric cols:', num_cols)\n",
        "print('Categorical cols:', cat_cols)\n"
      ],
      "id": "sMJZAX9xTQyZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cw_7knQTQya"
      },
      "source": [
        "## 18. Train/test split\n",
        "We keep a held-out test set for final evaluation.\n",
        "\n",
        "For professional healthcare machine learning applications, the optimal approach employs an 80-20 train-test split with mandatory stratification and a fixed random state, as this configuration ensures maximal training data utilization for model development while reserving sufficient samples for clinically meaningful validation; crucially, stratification preserves the real-world prevalence of medical conditions across both partitions, preventing skewed performance metrics that could lead to dangerous clinical misinterpretations, and the fixed random state guarantees reproducible research outcomes essential for regulatory compliance and patient safety in healthcare environments where model reliability directly impacts diagnostic accuracy and treatment decisions.\n",
        "\n"
      ],
      "id": "_cw_7knQTQya"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO031NtVTQyb"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)"
      ],
      "id": "CO031NtVTQyb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 19. Preprocessing pipelines\n",
        "Numeric: median imputation + standard scaling. Categorical: mode imputation + one-hot encoding.\n",
        "\n",
        "This preprocessing pipeline systematically transforms raw data into machine-learning-ready format by creating separate processing streams for numerical and categorical variables—handling missing values, scaling numerical features, and encoding categories—then seamlessly combines them into a unified feature set that preserves data integrity while making it compatible with ML algorithms.\n",
        "\n"
      ],
      "metadata": {
        "id": "KnoSqq8fd2NS"
      },
      "id": "KnoSqq8fd2NS"
    },
    {
      "cell_type": "code",
      "source": [
        "num_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "cat_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_pipeline, num_cols),\n",
        "    ('cat', cat_pipeline, cat_cols)\n",
        "])\n",
        "print('Preprocessor ready')"
      ],
      "metadata": {
        "id": "vIK1zzOid9N6"
      },
      "id": "vIK1zzOid9N6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73hFER02TQyf"
      },
      "source": [
        "# 20. Baseline models\n",
        "We'll train Logistic Regression (with class weight) and Random Forest. We'll also try a SMOTE pipeline.\n",
        "\n",
        "We are utilizing the our because:\n",
        "\n",
        "Logistic Regression → simple, interpretable baseline for binary classification.\n",
        "\n",
        "Random Forest → strong nonlinear model, robust and balanced.\n",
        "\n",
        "SMOTE + RF → fixes class imbalance effectively.\n",
        "\n",
        "XGBoost (Optuna) → powerful tuned model, often top performer for tabular data."
      ],
      "id": "73hFER02TQyf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20.1. Logistic Regression with class weighting"
      ],
      "metadata": {
        "id": "deA0LiYSWIby"
      },
      "id": "deA0LiYSWIby"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGt1Ic0VTQyh"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "lr_pipe = Pipeline([\n",
        "    ('pre', preprocessor),\n",
        "    ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42))\n",
        "])\n",
        "lr_pipe.fit(X_train, y_train)\n",
        "lr_probs = lr_pipe.predict_proba(X_test)[:,1]\n",
        "print('Logistic AUC:', roc_auc_score(y_test, lr_probs))\n",
        "# Random Forest without SMOTE\n",
        "rf_pipe = Pipeline([\n",
        "    ('pre', preprocessor),\n",
        "    ('clf', RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced'))\n",
        "])\n",
        "rf_pipe.fit(X_train, y_train)\n",
        "rf_probs = rf_pipe.predict_proba(X_test)[:,1]\n",
        "print('Random Forest AUC:', roc_auc_score(y_test, rf_probs))"
      ],
      "id": "qGt1Ic0VTQyh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcwHBWCBTQyi"
      },
      "source": [
        "## 21. SMOTE pipeline (oversampling) + Random Forest\n",
        "SMOTE operates only on numeric arrays; using imbalanced-learn's pipeline simplifies this.\n"
      ],
      "id": "gcwHBWCBTQyi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USGe0jwETQyj"
      },
      "outputs": [],
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "smote_pipe = ImbPipeline([\n",
        "    ('pre', preprocessor),\n",
        "    ('smote', smote),\n",
        "    ('clf', RandomForestClassifier(n_estimators=200, random_state=42))\n",
        "])\n",
        "smote_pipe.fit(X_train, y_train)\n",
        "smote_probs = smote_pipe.predict_proba(X_test)[:,1]\n",
        "print('SMOTE + RF AUC:', roc_auc_score(y_test, smote_probs))\n"
      ],
      "id": "USGe0jwETQyj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSkJnq5aTQyk"
      },
      "source": [
        "## 21. Evaluate and plot ROC and PR curves for the models\n"
      ],
      "id": "CSkJnq5aTQyk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Jtyy037TQyl"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    'Logistic': (lr_pipe, lr_probs),\n",
        "    'RandomForest': (rf_pipe, rf_probs),\n",
        "    'SMOTE_RandomForest': (smote_pipe, smote_probs)\n",
        "}\n",
        "plt.figure(figsize=(8,6))\n",
        "for name, (m, probs) in models.items():\n",
        "    fpr = np.nan\n",
        "    try:\n",
        "        from sklearn.metrics import roc_curve\n",
        "        fpr, tpr, _ = roc_curve(y_test, probs)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC={roc_auc:.3f})\")\n",
        "    except Exception as e:\n",
        "        print('Could not plot ROC for', name, e)\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves')\n",
        "plt.legend()\n",
        "fn = os.path.join(IMAGES_DIR, 'roc_curves.png')\n",
        "plt.savefig(fn, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Precision-Recall\n",
        "plt.figure(figsize=(8,6))\n",
        "for name, (m, probs) in models.items():\n",
        "    try:\n",
        "        precision, recall, _ = precision_recall_curve(y_test, probs)\n",
        "        pr_auc = auc(recall, precision)\n",
        "        plt.plot(recall, precision, label=f\"{name} (PR-AUC={pr_auc:.3f})\")\n",
        "    except Exception as e:\n",
        "        print('Could not plot PR for', name, e)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curves')\n",
        "plt.legend()\n",
        "fn = os.path.join(IMAGES_DIR, 'pr_curves.png')\n",
        "plt.savefig(fn, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "id": "8Jtyy037TQyl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDPVA_dJTQym"
      },
      "source": [
        "## 22. Confusion matrix & classification report for the best model (by AUC)\n"
      ],
      "id": "rDPVA_dJTQym"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8QL15HPTQym"
      },
      "outputs": [],
      "source": [
        "aucs = {name: roc_auc_score(y_test, probs) for name, (m, probs) in models.items()}\n",
        "best_name = max(aucs, key=aucs.get)\n",
        "print('AUCs:', aucs)\n",
        "print('Best model by AUC:', best_name)\n",
        "best_model = models[best_name][0]\n",
        "best_probs = models[best_name][1]\n",
        "best_preds = (best_probs >= 0.5).astype(int)\n",
        "cm = confusion_matrix(y_test, best_preds)\n",
        "print('Confusion matrix:\\n', cm)\n",
        "print('\\nClassification report:')\n",
        "print(classification_report(y_test, best_preds))\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Confusion Matrix - {best_name}')\n",
        "fn = os.path.join(IMAGES_DIR, f'confusion_matrix_{best_name}.png')\n",
        "plt.savefig(fn, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "id": "N8QL15HPTQym"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L1Tbdu2TQyn"
      },
      "source": [
        "## 23. Hyperparameter tuning with Optuna for XGBoost\n",
        "We will run a simple Optuna search to maximize cross-validated AUC. This is beginner-friendly but still powerful.\n"
      ],
      "id": "_L1Tbdu2TQyn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICKA7PFoTQyo"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 400),\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'use_label_encoder': False,\n",
        "        'eval_metric': 'logloss',\n",
        "        'random_state': 42\n",
        "    }\n",
        "    model = Pipeline([\n",
        "        ('pre', preprocessor),\n",
        "        ('clf', XGBClassifier(**params))\n",
        "    ])\n",
        "    # cross-validated AUC (stratified)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=StratifiedKFold(n_splits=4, shuffle=True, random_state=42), scoring='roc_auc')\n",
        "    return float(np.mean(scores))\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=25)\n",
        "print('Best trial:')\n",
        "print(study.best_trial.params)\n"
      ],
      "id": "ICKA7PFoTQyo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 24. Train a final XGBoost model with the best params"
      ],
      "metadata": {
        "id": "EVJO1mLST9Im"
      },
      "id": "EVJO1mLST9Im"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alvP5SA8TQyp"
      },
      "outputs": [],
      "source": [
        "# Train a final XGBoost model with the best params\n",
        "best_params = study.best_trial.params\n",
        "best_params.update({'use_label_encoder': False, 'eval_metric': 'logloss', 'random_state': 42})\n",
        "final_xgb = Pipeline([\n",
        "    ('pre', preprocessor),\n",
        "    ('clf', XGBClassifier(**best_params))\n",
        "])\n",
        "final_xgb.fit(X_train, y_train)\n",
        "xgb_probs = final_xgb.predict_proba(X_test)[:,1]\n",
        "print('Final XGB AUC:', roc_auc_score(y_test, xgb_probs))\n",
        "# Save model\n",
        "# Ensure the directory exists before saving\n",
        "models_dir = os.path.join('/mnt/data/heart-disease-project', 'models')\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "joblib.dump(final_xgb, os.path.join(models_dir, 'final_xgb.pkl'))\n",
        "print('Saved final model to:', os.path.join(models_dir, 'final_xgb.pkl'))"
      ],
      "id": "alvP5SA8TQyp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cJszaRpTQyp"
      },
      "source": [
        "## 10. Compare final XGBoost to previous best and save results\n"
      ],
      "id": "4cJszaRpTQyp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkO0EPQFTQyq"
      },
      "outputs": [],
      "source": [
        "finals = {\n",
        "    'XGBoost_Optuna': (final_xgb, xgb_probs)\n",
        "}\n",
        "all_models = {**models, **finals}\n",
        "for name, (m, probs) in all_models.items():\n",
        "\n",
        "\n",
        "    try:\n",
        "        precision, recall, _ = precision_recall_curve(y_test, probs)\n",
        "        print(f\"{name}: AUC={roc_auc_score(y_test, probs):.3f}, PR-AUC={auc(recall, precision):.3f}\")\n",
        "    except Exception as e:\n",
        "        print('Skipped metrics for', name, e)"
      ],
      "id": "pkO0EPQFTQyq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9IIxj_3TQyq"
      },
      "outputs": [],
      "source": [
        "# Save a simple CSV of model scores\n",
        "scores = []\n",
        "for name, (m, probs) in all_models.items():\n",
        "    try:\n",
        "        scores.append({'model': name, 'auc': roc_auc_score(y_test, probs)})\n",
        "    except:\n",
        "        pass\n",
        "scores_df = pd.DataFrame(scores).sort_values('auc', ascending=False)\n",
        "# Ensure the reports directory exists before saving\n",
        "reports_dir = os.path.join('/mnt/data/heart-disease-project', 'reports')\n",
        "os.makedirs(reports_dir, exist_ok=True)\n",
        "scores_df.to_csv(os.path.join(reports_dir, 'model_scores.csv'), index=False)\n",
        "display(scores_df)"
      ],
      "id": "X9IIxj_3TQyq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjnbegMmTQyr"
      },
      "source": [
        "## 25. Save ROC curve for final XGBoost\n"
      ],
      "id": "VjnbegMmTQyr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZgQLx2UTQyr"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, _ = roc_curve(y_test, xgb_probs)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(fpr, tpr, label=f'XGBoost (AUC={roc_auc_score(y_test, xgb_probs):.3f})')\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Final XGBoost ROC')\n",
        "plt.legend()\n",
        "fn = os.path.join(IMAGES_DIR, 'final_xgb_roc.png')\n",
        "plt.savefig(fn, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "id": "DZgQLx2UTQyr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WRQtyh8TQys"
      },
      "source": [
        "## 26. Save Precision-Recall for final XGBoost\n"
      ],
      "id": "5WRQtyh8TQys"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYI7DKyxTQys"
      },
      "outputs": [],
      "source": [
        "precision, recall, _ = precision_recall_curve(y_test, xgb_probs)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(recall, precision, label=f'XGBoost (PR-AUC={auc(recall, precision):.3f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Final XGBoost Precision-Recall')\n",
        "plt.legend()\n",
        "fn = os.path.join(IMAGES_DIR, 'final_xgb_pr.png')\n",
        "plt.savefig(fn, dpi=150, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "id": "kYI7DKyxTQys"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87xeHLQ4TQyt"
      },
      "source": [
        "## 27. Feature importance (from XGBoost) — simple approach\n",
        "We will extract feature names after preprocessing by applying the preprocessor to a sample and then retrieving feature names.\n"
      ],
      "id": "87xeHLQ4TQyt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-kMdvGHTQyu"
      },
      "outputs": [],
      "source": [
        "from sklearn.exceptions import NotFittedError\n",
        "# from sklearn.preprocessing import OneHotEncoder, StandardScaler # Already imported earlier\n",
        "# from sklearn.impute import SimpleImputer # Already imported earlier\n",
        "\n",
        "\n",
        "def get_feature_names(column_transformer):\n",
        "    \"\"\"Get feature names from a ColumnTransformer after fitting.\"\"\"\n",
        "    output_features = []\n",
        "    for name, transformer, original_features in column_transformer.transformers_:\n",
        "        # Skip if the list of original features for this transformer is empty\n",
        "        if not original_features:\n",
        "            print(f\"Info: Skipping transformer '{name}' as it is applied to an empty list of columns.\")\n",
        "            continue\n",
        "\n",
        "        if transformer == 'passthrough':\n",
        "            output_features.extend(original_features)\n",
        "        elif hasattr(transformer, 'get_feature_names_out'):\n",
        "            # For transformers with get_feature_names_out (like OneHotEncoder, StandardScaler, etc.)\n",
        "            try:\n",
        "                output_features.extend(transformer.get_feature_names_out(original_features))\n",
        "            except NotFittedError:\n",
        "                 # If the transformer isn't fitted yet, fallback to original names as a warning\n",
        "                 print(f\"Warning: Transformer {name} is not fitted. Using original column names: {original_features}\")\n",
        "                 output_features.extend(original_features)\n",
        "            except Exception as e:\n",
        "                 print(f\"Warning: Could not get feature names from transformer {name} using get_feature_names_out: {e}. Using original column names.\")\n",
        "                 output_features.extend(original_features)\n",
        "        elif hasattr(transformer, 'get_feature_names'):\n",
        "             # For older transformers with get_feature_names\n",
        "            try:\n",
        "                output_features.extend(transformer.get_feature_names(original_features))\n",
        "            except NotFittedError:\n",
        "                 print(f\"Warning: Transformer {name} is not fitted. Using original column names: {original_features}\")\n",
        "                 output_features.extend(original_features)\n",
        "            except Exception as e:\n",
        "                 print(f\"Warning: Could not get feature names from transformer {name} using get_feature_names: {e}. Using original column names.\")\n",
        "                 output_features.extend(original_features)\n",
        "\n",
        "        elif hasattr(transformer, 'named_steps'):\n",
        "            # For pipelines, try to get names from the last step if it has get_feature_names_out\n",
        "            last_step = list(transformer.named_steps.values())[-1]\n",
        "            if hasattr(last_step, 'get_feature_names_out'):\n",
        "                try:\n",
        "                    output_features.extend(last_step.get_feature_names_out(original_features))\n",
        "                except NotFittedError:\n",
        "                    print(f\"Warning: Last step of pipeline {name} is not fitted. Using original column names: {original_features}\")\n",
        "                    output_features.extend(original_features)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not get feature names from last step of pipeline {name} using get_feature_names_out: {e}. Using original column names.\")\n",
        "                    output_features.extend(original_features)\n",
        "            elif hasattr(last_step, 'get_feature_names'):\n",
        "                try:\n",
        "                    output_features.extend(last_step.get_feature_names(original_features))\n",
        "                except NotFittedError:\n",
        "                    print(f\"Warning: Last step of pipeline {name} is not fitted. Using original column names: {original_features}\")\n",
        "                    output_features.extend(original_features)\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Could not get feature names from last step of pipeline {name} using get_feature_names: {e}. Using original column names.\")\n",
        "                    output_features.extend(original_features)\n",
        "            else:\n",
        "                # Fallback for pipelines where the last step doesn't have feature names method\n",
        "                # If the last step is a transformer that doesn't change the number of features (like imputer or scaler),\n",
        "                # we can often assume the original features are the output features for this step of the CT.\n",
        "                if isinstance(last_step, (SimpleImputer, StandardScaler)):\n",
        "                     print(f\"Info: Using original features for pipeline step {name} (likely Imputer/Scaler).\")\n",
        "                     output_features.extend(original_features)\n",
        "                else:\n",
        "                     print(f\"Warning: Could not get detailed feature names from pipeline step {name}. Using original column names.\")\n",
        "                     output_features.extend(original_features)\n",
        "        else:\n",
        "            # Fallback for simple transformers without specific methods\n",
        "            print(f\"Warning: Could not get detailed feature names from transformer {name}. Using original column names.\")\n",
        "            output_features.extend(original_features)\n",
        "\n",
        "    return output_features\n",
        "\n",
        "# Fit the preprocessor before getting feature names\n",
        "# If preprocessor is already part of a fitted pipeline, this fit might not be strictly necessary\n",
        "# for accessing _fitted_ transformers, but it ensures the preprocessor itself is fitted.\n",
        "try:\n",
        "    preprocessor.fit(X_train)\n",
        "except Exception as e:\n",
        "    # This might happen if the preprocessor is already fitted within a pipeline\n",
        "    print(\"Preprocessor might be already fitted within the model pipeline:\", e)\n",
        "    pass # Assume preprocessor is fitted within the final_xgb pipeline\n",
        "\n",
        "\n",
        "# Get feature names from the fitted preprocessor within the final model pipeline\n",
        "# Access the fitted preprocessor from the final model pipeline\n",
        "fitted_preprocessor = final_xgb.named_steps['pre']\n",
        "feat_names = get_feature_names(fitted_preprocessor)\n",
        "\n",
        "\n",
        "try:\n",
        "    booster = final_xgb.named_steps['clf']\n",
        "    # xgboost feature importance uses original feature indices, but with pipeline we have transformed features\n",
        "    importances = booster.feature_importances_\n",
        "\n",
        "    # Ensure the number of feature importances matches the number of feature names\n",
        "    if len(importances) != len(feat_names):\n",
        "        print(f\"Error: Mismatch between number of feature importances ({len(importances)}) and feature names ({len(feat_names)}).\")\n",
        "        # Fallback to simpler naming or raise an error\n",
        "        fi_df = pd.DataFrame({'feature': [f'feature_{i}' for i in range(len(importances))], 'importance': importances})\n",
        "    else:\n",
        "        fi_df = pd.DataFrame({'feature': feat_names, 'importance': importances})\n",
        "\n",
        "    fi_df = fi_df.sort_values('importance', ascending=False).head(20)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.barplot(x='importance', y='feature', data=fi_df)\n",
        "    plt.title('Top 20 feature importances (XGBoost)')\n",
        "    fn = os.path.join(IMAGES_DIR, 'xgb_feature_importance.png')\n",
        "    plt.savefig(fn, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print('Could not compute feature importances or plot:', e)"
      ],
      "id": "R-kMdvGHTQyu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDLNUcWuTQyu"
      },
      "source": [
        "## 28. Save final model and report\n"
      ],
      "id": "NDLNUcWuTQyu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwTGoHd8TQyv"
      },
      "outputs": [],
      "source": [
        "# Define the base directory for saving models and reports\n",
        "base = '/content/gdrive/MyDrive/heart-disease-project'\n",
        "\n",
        "os.makedirs(os.path.join(base, 'models'), exist_ok=True)\n",
        "joblib.dump(final_xgb, os.path.join(base, 'models', 'final_xgb_optuna.pkl'))\n",
        "print('Saved final model to models/final_xgb_optuna.pkl')\n",
        "print('Modeling notebook complete. Review images in images/ and model scores in reports/model_scores.csv')"
      ],
      "id": "GwTGoHd8TQyv"
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "FluYysAX5yoe"
      },
      "id": "FluYysAX5yoe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28ccf5f3"
      },
      "source": [
        "# 29 - Interpretation\n",
        "\n",
        "This notebook explains how the best model makes its predictions.\n",
        "- Uses **SHAP** for global + local feature importance.\n",
        "- Optionally uses **LIME** for local instance interpretation.\n",
        "- Saves interpretability plots to `/images/`.\n"
      ],
      "id": "28ccf5f3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0297de22"
      },
      "outputs": [],
      "source": [
        "# --- Imports ---\n",
        "import pandas as pd\n",
        "import shap\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load processed data and model\n",
        "df = pd.read_csv('../data/processed/heart_clean.csv')\n",
        "model = joblib.load('../models/final_xgb_optuna.pkl')\n",
        "\n",
        "X = df.drop(columns=['target'])\n",
        "y = df['target']\n",
        "print('Data shape:', X.shape)\n"
      ],
      "id": "0297de22"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59a132d8"
      },
      "source": [
        "!pip install lime"
      ],
      "id": "59a132d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dff2fc4"
      },
      "outputs": [],
      "source": [
        "# --- SHAP Interpretation ---\n",
        "# SHAP helps explain global feature importance and per-patient explanations.\n",
        "explainer = shap.Explainer(model, X)\n",
        "shap_values = explainer(X)\n",
        "\n",
        "# Global summary plot\n",
        "plt.title('SHAP Summary Plot')\n",
        "shap.summary_plot(shap_values, X, show=False)\n",
        "plt.savefig('../images/shap_summary.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Bar plot (mean absolute value)\n",
        "shap.summary_plot(shap_values, X, plot_type='bar', show=False)\n",
        "plt.title('Mean Absolute SHAP Values')\n",
        "plt.savefig('../images/shap_importance_bar.png', bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "id": "3dff2fc4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4d885dd"
      },
      "outputs": [],
      "source": [
        "# --- Dependence Plot for Top Features ---\n",
        "top_feature = X.columns[0]\n",
        "shap.dependence_plot(top_feature, shap_values.values, X, show=False)\n",
        "plt.title(f'SHAP Dependence for {top_feature}')\n",
        "plt.savefig(f'../images/shap_dependence_{top_feature}.png', bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "id": "e4d885dd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f24efad"
      },
      "outputs": [],
      "source": [
        "# --- LIME Example (optional, for one patient) ---\n",
        "import numpy as np\n",
        "explainer_lime = LimeTabularExplainer(\n",
        "    training_data=np.array(X),\n",
        "    feature_names=X.columns,\n",
        "    class_names=['No Disease', 'Disease'],\n",
        "    mode='classification'\n",
        ")\n",
        "\n",
        "# Pick one random patient\n",
        "i = np.random.randint(0, len(X))\n",
        "exp = explainer_lime.explain_instance(X.iloc[i].values, model.predict_proba)\n",
        "exp.save_to_file('../images/lime_patient_example.html')\n",
        "print(f'LIME explanation saved for patient #{i}')\n"
      ],
      "id": "2f24efad"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7671a7c6"
      },
      "outputs": [],
      "source": [
        "# --- Fairness / Subgroup Comparison (if demographic columns exist) ---\n",
        "if any(col in X.columns for col in ['sex', 'gender', 'age']):\n",
        "    print('Running subgroup comparison...')\n",
        "    if 'sex' in X.columns:\n",
        "        male_auc = roc_auc_score(y[X['sex']==1], model.predict_proba(X[X['sex']==1])[:,1])\n",
        "        female_auc = roc_auc_score(y[X['sex']==0], model.predict_proba(X[X['sex']==0])[:,1])\n",
        "        print(f'AUC Male: {male_auc:.3f}, Female: {female_auc:.3f}')\n",
        "    if 'age' in X.columns:\n",
        "        median_age = X['age'].median()\n",
        "        young_auc = roc_auc_score(y[X['age']<=median_age], model.predict_proba(X[X['age']<=median_age])[:,1])\n",
        "        old_auc = roc_auc_score(y[X['age']>median_age], model.predict_proba(X[X['age']>median_age])[:,1])\n",
        "        print(f'AUC Younger vs Older: {young_auc:.3f} / {old_auc:.3f}')\n"
      ],
      "id": "7671a7c6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32c90923"
      },
      "outputs": [],
      "source": [
        "# --- Summary of Insights ---\n",
        "print('\\nKey Insights:')\n",
        "print('- Features like age, cholesterol, and chest pain type typically show strong SHAP values.')\n",
        "print('- Positive SHAP value → higher heart disease risk contribution.')\n",
        "print('- LIME helps explain individual predictions in an easy-to-read format.')\n",
        "print('- Fairness metrics suggest whether the model behaves similarly across groups.')\n"
      ],
      "id": "32c90923"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a652c2fa"
      },
      "source": [
        "# 30. SHAP explainability (global + local)\n",
        "This cell assumes you have a fitted pipeline named `clf` and `X_train`/`X_test`."
      ],
      "id": "a652c2fa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fc7e280"
      },
      "outputs": [],
      "source": [
        "\n",
        "# SHAP analysis (global + local)\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# Ensure model exists\n",
        "if 'clf' not in globals():\n",
        "    print('Warning: clf not found. Please load or train your pipeline and ensure it is named `clf`.')\n",
        "else:\n",
        "    model = clf.named_steps.get('model', None)\n",
        "    preprocessor = clf.named_steps.get('preprocessor', None)\n",
        "    if model is None or preprocessor is None:\n",
        "        print('clf does not have expected steps preprocessor/model.')\n",
        "    else:\n",
        "        # Use a small sample for SHAP to keep runtime low\n",
        "        X_sample = X_train.sample(n=min(200, len(X_train)), random_state=RANDOM_STATE)\n",
        "        X_pre = preprocessor.transform(X_sample)\n",
        "        try:\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            shap_values = explainer.shap_values(X_pre)\n",
        "            try:\n",
        "                shap.summary_plot(shap_values, X_pre)\n",
        "            except Exception:\n",
        "                shap.summary_plot(shap_values, X_pre, feature_names=getattr(preprocessor, 'get_feature_names_out', None) and preprocessor.get_feature_names_out())\n",
        "        except Exception as e:\n",
        "            print('TreeExplainer failed, falling back to KernelExplainer:', e)\n",
        "            try:\n",
        "                background = X_pre[:50]\n",
        "                explainer = shap.KernelExplainer(lambda x: model.predict_proba(x)[:,1] if hasattr(model, 'predict_proba') else model.predict(x), background)\n",
        "                shap_values = explainer.shap_values(X_pre[:50])\n",
        "                shap.summary_plot(shap_values, X_pre[:50])\n",
        "            except Exception as e2:\n",
        "                print('KernelExplainer also failed:', e2)\n"
      ],
      "id": "2fc7e280"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}